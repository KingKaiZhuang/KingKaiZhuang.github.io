<!DOCTYPE html><html lang="zh-TW" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Training a Classifier | StackPenguin</title><meta name="author" content="StackPenguin"><meta name="copyright" content="StackPenguin"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="程式碼與資料參考來源：https:&#x2F;&#x2F;docs.pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;blitz&#x2F;cifar10_tutorial.html#define-a-convolutional-neural-network 123import torchimport torchvisionimport torchvision.transforms as transforms  1">
<meta property="og:type" content="article">
<meta property="og:title" content="Training a Classifier">
<meta property="og:url" content="https://stackpenguin.com/2025/10/29/training-a-classifier/index.html">
<meta property="og:site_name" content="StackPenguin">
<meta property="og:description" content="程式碼與資料參考來源：https:&#x2F;&#x2F;docs.pytorch.org&#x2F;tutorials&#x2F;beginner&#x2F;blitz&#x2F;cifar10_tutorial.html#define-a-convolutional-neural-network 123import torchimport torchvisionimport torchvision.transforms as transforms  1">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="https://stackpenguin.com/images/Pytorch.png">
<meta property="article:published_time" content="2025-10-28T16:00:00.000Z">
<meta property="article:modified_time" content="2026-01-28T03:08:27.796Z">
<meta property="article:author" content="StackPenguin">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://stackpenguin.com/images/Pytorch.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Training a Classifier",
  "url": "https://stackpenguin.com/2025/10/29/training-a-classifier/",
  "image": "https://stackpenguin.com/images/Pytorch.png",
  "datePublished": "2025-10-28T16:00:00.000Z",
  "dateModified": "2026-01-28T03:08:27.796Z",
  "author": [
    {
      "@type": "Person",
      "name": "StackPenguin",
      "url": "https://stackpenguin.com"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://stackpenguin.com/2025/10/29/training-a-classifier/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '複製成功',
    error: '複製失敗',
    noSupport: '瀏覽器不支援'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '剛剛',
    min: '分鐘前',
    hour: '小時前',
    day: '天前',
    month: '個月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '載入更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Training a Classifier',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/wave.css"><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 7.3.0"></head><body><script>window.paceOptions = {
  restartOnPushState: false
}

btf.addGlobalFn('pjaxSend', () => {
  Pace.restart()
}, 'pace_restart')

</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@latest/themes/blue/pace-theme-flash.css"/><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/images/Avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">標籤</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分類</div><div class="length-num">35</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/images/Pytorch.png);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">StackPenguin</span></a><a class="nav-page-title" href="/"><span class="site-name">Training a Classifier</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首頁</span></span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 時間軸</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">Training a Classifier</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">發表於</span><time class="post-meta-date-created" datetime="2025-10-28T16:00:00.000Z" title="發表於 2025-10-29 00:00:00">2025-10-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新於</span><time class="post-meta-date-updated" datetime="2026-01-28T03:08:27.796Z" title="更新於 2026-01-28 11:08:27">2026-01-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/">ai</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/ai/pytorch/">pytorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">瀏覽量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>程式碼與資料參考來源：<a target="_blank" rel="noopener" href="https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-convolutional-neural-network">https://docs.pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#define-a-convolutional-neural-network</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">import torchvision.transforms as transforms</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torchvision</span><br><span class="line">import torchvision.transforms as transforms</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">batch_size = 4</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True,</span><br><span class="line">                                        download=True, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,</span><br><span class="line">                                          shuffle=True, num_workers=2)</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=False,</span><br><span class="line">                                       download=True, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,</span><br><span class="line">                                         shuffle=False, num_workers=2)</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">classes = (&#x27;plane&#x27;, &#x27;car&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;,</span><br><span class="line">           &#x27;deer&#x27;, &#x27;dog&#x27;, &#x27;frog&#x27;, &#x27;horse&#x27;, &#x27;ship&#x27;, &#x27;truck&#x27;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose(</span><br><span class="line">    [transforms.ToTensor(),</span><br><span class="line">     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])</span><br><span class="line"></span><br><span class="line">batch_size = 4</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=True,</span><br><span class="line">                                        download=True, transform=transform)</span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,</span><br><span class="line">                                          shuffle=True, num_workers=2)</span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=&#x27;./data&#x27;, train=False,</span><br><span class="line">                                       download=True, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,</span><br><span class="line">                                         shuffle=False, num_workers=2)</span><br><span class="line"></span><br><span class="line">classes = (&#x27;plane&#x27;, &#x27;car&#x27;, &#x27;bird&#x27;, &#x27;cat&#x27;,</span><br><span class="line">           &#x27;deer&#x27;, &#x27;dog&#x27;, &#x27;frog&#x27;, &#x27;horse&#x27;, &#x27;ship&#x27;, &#x27;truck&#x27;)</span><br></pre></td></tr></table></figure>

<p>這段程式主要的目的，是建立一個可以讓 PyTorch 模型方便使用的影像資料載入流程。首先，程式定義了一個名為 <code>transform</code> 的轉換流程，這個轉換由兩個步驟組成。第一個步驟 <code>transforms.ToTensor()</code> 是把原本的影像（可能是 PIL Image 或 numpy 陣列）轉成 PyTorch 可以處理的 Tensor 格式，並且把像素值從 0～255 縮放到 0～1 之間。第二個步驟 <code>transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))</code> 則是將每個顏色通道進行標準化，把影像的數值範圍轉換成 -1 到 1 之間，這樣能讓模型在訓練時更穩定、更容易收斂。</p>
<p>接著，<code>batch_size = 4</code> 表示每次從資料集中取出四張圖片，組成一個小批次（batch）來訓練。這樣做可以減少記憶體使用量，並加快訓練速度。</p>
<p>在建立訓練資料集時，使用了 <code>torchvision.datasets.CIFAR10</code> 這個內建的資料集。CIFAR-10 是一個常見的影像分類資料集，裡面有 10 種不同類別的物體（例如飛機、汽車、貓、狗等）。參數 <code>root=&#39;./data&#39;</code> 代表資料會被存放在專案資料夾中的 <code>data</code> 目錄下；<code>train=True</code> 指定要載入訓練資料；<code>download=True</code> 讓程式在本地沒有資料時自動從網路下載；而 <code>transform=transform</code> 則確保每張影像在載入時都會先經過剛剛定義的轉換流程。</p>
<p>建立好訓練資料集後，<code>trainloader</code> 透過 <code>torch.utils.data.DataLoader</code> 將資料集包裝成可以分批次讀取的形式。這個物件會在訓練時自動提供圖片與標籤。<code>shuffle=True</code> 表示在每個訓練輪次（epoch）開始前，資料都會被隨機打亂，避免模型記住資料的順序而影響學習。<code>num_workers=2</code> 則是設定使用兩個背景執行緒加快資料載入的速度。</p>
<p>測試資料的部分幾乎相同，只是 <code>train=False</code> 代表這次載入的是測試資料集，並且 <code>shuffle=False</code>，確保測試資料的順序固定，方便在評估模型時對照結果。</p>
<p>最後，<code>classes</code> 定義了一個包含 10 個字串的元組，對應到 CIFAR-10 資料集的十個分類名稱，分別是飛機（plane）、汽車（car）、鳥（bird）、貓（cat）、鹿（deer）、狗（dog）、青蛙（frog）、馬（horse）、船（ship）以及卡車（truck）。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line"># functions to show an image</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">def imshow(img):</span><br><span class="line">    img = img / 2 + 0.5     # unnormalize</span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (1, 2, 0)))</span><br><span class="line">    plt.show()</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line"></span><br><span class="line"># get some random training images</span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = next(dataiter)</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line"># show images</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"># print labels</span><br><span class="line">print(&#x27; &#x27;.join(f&#x27;&#123;classes[labels[j]]:5s&#125;&#x27; for j in range(batch_size)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># functions to show an image</span><br><span class="line"></span><br><span class="line">def imshow(img):</span><br><span class="line">    img = img / 2 + 0.5     # unnormalize</span><br><span class="line">    npimg = img.numpy()</span><br><span class="line">    plt.imshow(np.transpose(npimg, (1, 2, 0)))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"># get some random training images</span><br><span class="line">dataiter = iter(trainloader)</span><br><span class="line">images, labels = next(dataiter)</span><br><span class="line"></span><br><span class="line"># show images</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line"># print labels</span><br><span class="line">print(&#x27; &#x27;.join(f&#x27;&#123;classes[labels[j]]:5s&#125;&#x27; for j in range(batch_size)))</span><br></pre></td></tr></table></figure>

<p>這段程式的主要功能是從訓練資料集中隨機取出一批圖片，將它們組合起來顯示在螢幕上，並同時印出每張圖片所對應的分類名稱。它常用於訓練前的資料檢查，讓我們確認影像資料是否被正確載入與轉換。</p>
<p>首先，程式匯入了 <code>matplotlib.pyplot</code> 和 <code>numpy</code>，前者用來繪製和顯示圖片，後者則是進行陣列處理。接著定義了一個名為 <code>imshow</code> 的函式，用來顯示影像。由於在資料載入時影像經過標準化（值從 <code>[0,1]</code> 變成 <code>[-1,1]</code>），因此在顯示之前需要進行反標準化，也就是 <code>img = img / 2 + 0.5</code> 這一行，把影像還原到可視範圍 <code>[0,1]</code>。接著 <code>npimg = img.numpy()</code> 將 PyTorch 的 Tensor 轉成 NumPy 陣列，因為 Matplotlib 只能處理 NumPy 格式的資料。再來，<code>np.transpose(npimg, (1, 2, 0))</code> 這行是把影像的維度順序從 <code>(channels, height, width)</code> 調整成 Matplotlib 所需要的 <code>(height, width, channels)</code>。最後透過 <code>plt.imshow()</code> 顯示圖片，<code>plt.show()</code> 則把圖形真正畫出來。</p>
<p>在顯示影像之前，程式會先從訓練資料中取出一批圖片。這裡 <code>dataiter = iter(trainloader)</code> 建立一個可迭代物件，用來從資料載入器中取得資料，而 <code>images, labels = next(dataiter)</code> 則是取出下一批資料。這一批資料包含了四張圖片（因為之前設定 <code>batch_size = 4</code>），以及這四張圖片的標籤。</p>
<p><img src="/images/image-27.png"></p>
<p>接著使用 <code>torchvision.utils.make_grid(images)</code> 將多張圖片排成一張網格，方便一次顯示多張影像。這個網格會被傳入 <code>imshow()</code> 函式中顯示出來。最後一行 <code>print(&#39; &#39;.join(f&#39;&#123;classes[labels[j]]:5s&#125;&#39; for j in range(batch_size)))</code> 則是依序把每張圖片的類別名稱印出來。它會根據 <code>labels</code> 取得每張圖片的類別代號，再利用 <code>classes</code> 對應到實際的名稱（例如 “cat”、”dog”、”car” 等），並用空格分隔後印在同一行。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(3, 6, 5)</span><br><span class="line">        self.pool = nn.MaxPool2d(2, 2)</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 5)</span><br><span class="line">        self.fc1 = nn.Linear(16 * 5 * 5, 120)</span><br><span class="line">        self.fc2 = nn.Linear(120, 84)</span><br><span class="line">        self.fc3 = nn.Linear(84, 10)</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = torch.flatten(x, 1) # flatten all dimensions except batch</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(3, 6, 5)</span><br><span class="line">        self.pool = nn.MaxPool2d(2, 2)</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 5)</span><br><span class="line">        self.fc1 = nn.Linear(16 * 5 * 5, 120)</span><br><span class="line">        self.fc2 = nn.Linear(120, 84)</span><br><span class="line">        self.fc3 = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = torch.flatten(x, 1) # flatten all dimensions except batch</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br></pre></td></tr></table></figure>

<p>這段程式定義並建立了一個基本的卷積神經網路（Convolutional Neural Network, CNN），用來進行影像分類。這個網路的結構與 LeNet 類似，是一個簡單但經典的 CNN 範例，常用於 CIFAR-10 這類小型影像資料集的學習實驗。</p>
<p>首先，程式匯入了 <code>torch.nn</code> 與 <code>torch.nn.functional</code>。前者提供各種神經網路層的模組，例如卷積層、線性層（全連接層）等；後者則提供常用的函式操作，例如激活函式 ReLU（Rectified Linear Unit）等。</p>
<p>接著，透過定義一個名為 <code>Net</code> 的類別來建立網路結構。這個類別繼承自 <code>nn.Module</code>，是 PyTorch 中所有模型的基底類別。<code>__init__</code> 方法中定義了網路的各個層。<code>self.conv1 = nn.Conv2d(3, 6, 5)</code> 是第一個卷積層，它的輸入有三個通道（對應到彩色影像的 R、G、B 三色），輸出六個特徵圖（feature maps），而卷積核（filter）的大小是 5×5。接著 <code>self.pool = nn.MaxPool2d(2, 2)</code> 是最大池化層，用於縮小影像尺寸，減少參數與計算量。<code>self.conv2 = nn.Conv2d(6, 16, 5)</code> 是第二個卷積層，輸入為六個通道、輸出十六個特徵圖，同樣使用 5×5 的卷積核。</p>
<p>卷積層之後是三個全連接層（fully connected layers），也就是傳統神經網路的部分。<code>self.fc1 = nn.Linear(16 * 5 * 5, 120)</code> 代表第一個全連接層的輸入為 16×5×5 維的向量（也就是前面卷積與池化之後展平的結果），輸出為 120 個神經元。接著的 <code>self.fc2 = nn.Linear(120, 84)</code> 再將輸入從 120 維轉成 84 維，而最後一層 <code>self.fc3 = nn.Linear(84, 10)</code> 則輸出 10 維的結果，對應到 CIFAR-10 的十個分類類別。</p>
<p>在 <code>forward</code> 方法中定義了資料的前向傳遞過程，也就是影像如何一步步通過網路。首先，輸入 <code>x</code> 經過第一個卷積層 <code>self.conv1</code>，然後經過 ReLU 激活函式轉換，再經過最大池化層縮小尺寸。接著進入第二個卷積層 <code>self.conv2</code>，同樣再經過 ReLU 與池化層。這兩次卷積與池化之後，影像的空間維度大幅縮小，但特徵變得更抽象、更具辨識能力。接下來使用 <code>torch.flatten(x, 1)</code> 將多維的特徵圖展平成一維向量（除了批次維度外），使其能夠輸入到全連接層。然後依序通過兩層帶有 ReLU 激活函式的全連接層，最後輸出到 <code>fc3</code>，產生最終的分類結果。</p>
<p>最後一行 <code>net = Net()</code> 則是實際建立這個網路的實例。建立後的 <code>net</code> 物件就是一個可供訓練的 CNN 模型，能夠接收 CIFAR-10 的影像作為輸入，並輸出十個類別的預測結果。整體而言，這段程式展示了從卷積層、池化層到全連接層的完整 CNN 結構，構成了一個典型的影像分類模型。</p>
<h3 id="補充-ReLU-的作用"><a href="#補充-ReLU-的作用" class="headerlink" title="補充 : ReLU 的作用"></a>補充 : ReLU 的作用</h3><p><img src="/images/image-28.png"></p>
<p>它會<strong>把所有負值剪掉，正值保留</strong>。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</span><br></pre></td></tr></table></figure>

<p>這段程式的主要功能，是為神經網路設定<strong>損失函數（loss function）與優化器（optimizer）</strong>，也就是訓練模型時用來「評估表現」與「更新權重」的兩個關鍵步驟。它們就像訓練過程中的「老師」和「調整機制」，共同決定模型如何從錯誤中學習。</p>
<p>首先，<code>criterion = nn.CrossEntropyLoss()</code> 這行定義了損失函數，使用的是交叉熵損失（Cross Entropy Loss）。交叉熵是一種在<strong>分類問題</strong>中最常見的損失函數，它用來衡量模型預測的機率分佈與真實答案之間的差距。舉例來說，若模型預測一張圖片是「貓」的機率為 0.9，而真實標籤確實是貓，那損失值就會很小；但如果模型預測是「狗」的機率為 0.9，損失值就會很大。這樣模型在訓練時會自動調整權重，讓正確類別的機率越來越高。CrossEntropyLoss 在 PyTorch 中同時結合了 <strong>softmax</strong>（將輸出轉為機率）與 <strong>log loss</strong>（計算對數誤差），因此不需要手動再對輸出做 softmax 處理。</p>
<p>接著，<code>optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)</code> 這行設定了優化器，使用的是隨機梯度下降法（Stochastic Gradient Descent, SGD）。在訓練神經網路時，模型會根據損失函數的值計算出梯度，表示權重應該如何調整以降低損失。SGD 就是透過這些梯度逐步更新模型的參數。<code>net.parameters()</code> 表示要讓優化器管理網路中所有可學習的權重；<code>lr=0.001</code> 則是設定學習率（learning rate），也就是每次更新權重時的步伐大小。學習率太大會導致訓練不穩定、損失值震盪；太小則會讓收斂速度變慢。最後的 <code>momentum=0.9</code> 是動量參數，它能幫助模型在訓練過程中更順利地前進，減少在局部最小值附近震盪的情況。動量的概念就像 推球一樣，當球在斜坡上滾動時，會保留部分前一次的速度，讓更新方向更平滑、加速收斂。</p>
<p>總結來說，這段程式的作用是建立訓練模型時的「學習機制」：交叉熵損失負責告訴網路「預測錯了多少」，而 SGD 優化器則根據這些錯誤去微調權重，讓模型一步步學會更準確地分類影像。</p>
<h2 id="Train-the-network"><a href="#Train-the-network" class="headerlink" title="Train the network"></a>Train the network</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">for epoch in range(2):  # loop over the dataset multiple times</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for i, data in enumerate(trainloader, 0):</span><br><span class="line">        # get the inputs; data is a list of [inputs, labels]</span><br><span class="line">        inputs, labels = data</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">        # zero the parameter gradients</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">        # forward + backward + optimize</span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">        # print statistics</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        if i % 2000 == 1999:    # print every 2000 mini-batches</span><br><span class="line">            print(f&#x27;[&#123;epoch + 1&#125;, &#123;i + 1:5d&#125;] loss: &#123;running_loss / 2000:.3f&#125;&#x27;)</span><br><span class="line">            running_loss = 0.0</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">print(&#x27;Finished Training&#x27;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">for epoch in range(2):  # loop over the dataset multiple times</span><br><span class="line"></span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for i, data in enumerate(trainloader, 0):</span><br><span class="line">        # get the inputs; data is a list of [inputs, labels]</span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        # zero the parameter gradients</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        # forward + backward + optimize</span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        # print statistics</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        if i % 2000 == 1999:    # print every 2000 mini-batches</span><br><span class="line">            print(f&#x27;[&#123;epoch + 1&#125;, &#123;i + 1:5d&#125;] loss: &#123;running_loss / 2000:.3f&#125;&#x27;)</span><br><span class="line">            running_loss = 0.0</span><br><span class="line"></span><br><span class="line">print(&#x27;Finished Training&#x27;)</span><br></pre></td></tr></table></figure>

<p>這段程式是整個模型訓練過程的核心，它讓神經網路能夠不斷從資料中學習、修正錯誤，進而提升分類準確率。最外層的 <code>for epoch in range(2):</code> 代表訓練要進行兩個回合（epoch），也就是讓模型將整個訓練資料集完整學習兩次。每個 epoch 開始前，先將 <code>running_loss</code> 設為 0，這個變數用來累積損失值，以便在訓練過程中觀察模型表現是否逐步改善。</p>
<p>在每個 epoch 內部，<code>for i, data in enumerate(trainloader, 0):</code> 會從訓練資料載入器（<code>trainloader</code>）中依序取出資料批次（mini-batch）。每次取出的 <code>data</code> 由兩個部分組成：<code>inputs</code> 是輸入影像的張量（tensor），而 <code>labels</code> 是對應的真實分類標籤。由於前面設定過 <code>batch_size = 4</code>，因此每個批次會包含四張圖片與它們的標籤。</p>
<p>接著，<code>optimizer.zero_grad()</code> 是一個非常重要的步驟，用來清除前一次訓練所累積的梯度（gradient）。在 PyTorch 中，梯度會自動累加，若不清零，前一批的梯度會干擾新的更新，使模型權重被錯誤地調整。清除梯度之後，就能安全地開始新的學習迭代。</p>
<p>在這之後是整個學習流程的三個主要階段：前向傳播（forward）、反向傳播（backward）與參數更新（optimize）。<code>outputs = net(inputs)</code> 代表把輸入圖片送進神經網路中進行前向運算，網路會輸出一組預測結果。接著，<code>loss = criterion(outputs, labels)</code> 使用損失函數計算預測值與真實標籤之間的差距，損失值越大代表模型預測越不準確。<code>loss.backward()</code> 則是反向傳播的步驟，PyTorch 會根據損失值自動計算每個參數的梯度，也就是「模型應該怎麼修正自己」。最後，<code>optimizer.step()</code> 根據這些梯度更新模型的權重，使得網路在下一次預測時能更接近正確答案。</p>
<p>在每次迭代結束後，<code>running_loss += loss.item()</code> 會把這一批的損失值加總起來。為了方便觀察訓練進展，程式設計成每訓練 2000 批資料時，就會印出一次平均損失值：<code>print(f&#39;[&#123;epoch + 1&#125;, &#123;i + 1:5d&#125;] loss: &#123;running_loss / 2000:.3f&#125;&#39;)</code>。這樣可以直觀看出損失值是否持續下降，進而判斷模型是否正在有效地學習。印出後，<code>running_loss</code> 會被重置為 0，準備記錄下一段訓練的結果。</p>
<p>當外層的迴圈結束後，代表模型已經完成所有訓練回合，<code>print(&#39;Finished Training&#39;)</code> 會顯示訓練完成的訊息。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PATH = &#x27;./cifar_net.pth&#x27;</span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PATH = &#x27;./cifar_net.pth&#x27;</span><br><span class="line">torch.save(net.state_dict(), PATH)</span><br></pre></td></tr></table></figure>

<p>首先，<code>PATH = &#39;./cifar_net.pth&#39;</code> 是設定模型要儲存的路徑與檔案名稱。這裡使用的 <code>&#39;./cifar_net.pth&#39;</code> 代表在目前的工作目錄下建立一個名為 <strong>cifar_net.pth</strong> 的檔案。副檔名 <code>.pth</code> 是 PyTorch 模型檔案的常見命名方式，代表「parameters of torch」（意即儲存的是模型的參數）。</p>
<p>接下來的 <code>torch.save(net.state_dict(), PATH)</code> 是實際執行儲存的動作。這裡的 <code>net</code> 是前面訓練完成的神經網路，而 <code>net.state_dict()</code> 是一個特殊的字典（dictionary），裡面儲存了模型中所有層的<strong>參數權重（weights）與偏差值（biases）</strong>。換句話說，它只會保存模型「學到的知識」，而不是整個模型結構。這樣的設計讓儲存檔案更小、更靈活，也方便在不同環境中重新載入。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = next(dataiter)</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line"># print images</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(&#x27;GroundTruth: &#x27;, &#x27; &#x27;.join(f&#x27;&#123;classes[labels[j]]:5s&#125;&#x27; for j in range(4)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = next(dataiter)</span><br><span class="line"></span><br><span class="line"># print images</span><br><span class="line">imshow(torchvision.utils.make_grid(images))</span><br><span class="line">print(&#x27;GroundTruth: &#x27;, &#x27; &#x27;.join(f&#x27;&#123;classes[labels[j]]:5s&#125;&#x27; for j in range(4)))</span><br></pre></td></tr></table></figure>

<p>這段程式的功能是從測試資料集中隨機取出幾張圖片，顯示出它們的內容，並印出每張圖片的真實標籤（Ground Truth）。這通常是訓練完成後的第一個步驟，用來檢查測試資料是否正確載入，也方便之後與模型的預測結果進行比較。</p>
<p>首先，<code>dataiter = iter(testloader)</code> 用來建立一個可迭代的資料讀取器。<code>testloader</code> 是先前設定好的測試資料載入器，裡面包含了 CIFAR-10 的測試圖片與對應標籤。接著，<code>images, labels = next(dataiter)</code> 從這個載入器中取出一批資料。這一批包含兩部分：<code>images</code> 是四張測試圖片的影像張量（tensor），<code>labels</code> 則是這四張圖片的真實類別標籤，以數字形式儲存。例如，若某張圖片的標籤是 3，就代表它屬於 CIFAR-10 類別中的「cat」。</p>
<p>接下來的 <code>imshow(torchvision.utils.make_grid(images))</code> 是用來顯示圖片。由於這次取出的 <code>images</code> 是四張影像，因此使用 <code>torchvision.utils.make_grid(images)</code> 先把這四張圖排成一張大圖（grid），再交給 <code>imshow()</code> 函式顯示出來。<code>imshow()</code> 會先將影像資料反正規化（把像素值從 -1<del>1 的範圍還原為 0</del>1），然後利用 Matplotlib 將這張圖片網格畫出來。執行這行後，畫面上會出現四張並排的小圖片，也就是接下來模型要進行預測的測試樣本。</p>
<p><img src="/images/CleanShot-2025-10-29-at-11.41.16@2x-1024x385.png"></p>
<p>最後一行 <code>print(&#39;GroundTruth: &#39;, &#39; &#39;.join(f&#39;&#123;classes[labels[j]]:5s&#125;&#39; for j in range(4)))</code> 則負責印出這四張圖片的真實類別名稱。這裡的 <code>classes</code> 是先前定義好的 CIFAR-10 類別名稱列表，例如 <code>(&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;)</code>。<code>labels[j]</code> 會取出每張圖片的數字標籤，而 <code>classes[labels[j]]</code> 則將這些數字轉換為對應的文字名稱。<code>&#39; &#39;.join(...)</code> 用來把所有類別名稱以空格串起來，讓輸出更整齊。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH, weights_only=True))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = Net()</span><br><span class="line">net.load_state_dict(torch.load(PATH, weights_only=True))</span><br></pre></td></tr></table></figure>

<p>這段程式的主要功能是<strong>重新載入先前訓練並儲存好的模型權重</strong>，讓模型能夠直接使用訓練成果，而不必重新訓練。它通常出現在「模型測試」或「實際應用」的階段。</p>
<p>首先，<code>net = Net()</code> 這行代表建立一個新的模型物件。這個 <code>Net</code> 是先前定義好的神經網路結構類別（包含卷積層、池化層、全連接層等）。這裡建立的 <code>net</code> 是一個「空白的模型骨架」，也就是只有網路的架構，尚未包含任何已學到的參數或權重。</p>
<p>接下來的 <code>net.load_state_dict(torch.load(PATH, weights_only=True))</code> 是將儲存在檔案中的權重載入到這個模型中。前面的 <code>torch.load(PATH, weights_only=True)</code> 是從先前儲存的檔案（例如 <code>&#39;./cifar_net.pth&#39;</code>）中讀取模型參數。這個檔案是在訓練結束後用 <code>torch.save(net.state_dict(), PATH)</code> 儲存的，因此裡面包含了每一層神經網路的權重（weights）與偏差（biases）數值。</p>
<p><code>load_state_dict()</code> 則是 PyTorch 專門用來把這些儲存的參數載回模型的函式。當這個函式被呼叫時，它會根據模型層的名稱，自動將檔案中的權重一一對應回模型中相同名稱的層，讓這個新的 <code>net</code> 恢復成與訓練完成時相同的狀態。參數 <code>weights_only=True</code> 則指定只讀取權重數據，而不載入其他不必要的資訊（例如訓練時的設定或暫存變數），這樣可以確保載入過程簡潔且不受版本影響。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(images)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs = net(images)</span><br></pre></td></tr></table></figure>

<p><code>images</code> 是一批（batch）要測試的影像資料，通常是從 <code>testloader</code> 中取出來的四張圖片（因為你先前設定 <code>batch_size = 4</code>）。這些影像已經是經過 <code>transforms.ToTensor()</code> 和 <code>transforms.Normalize()</code> 處理的張量（tensor），形狀通常是 <code>(4, 3, 32, 32)</code>，分別代表：4 張圖片、3 個顏色通道（RGB）、以及 32×32 的解析度。</p>
<p><code>net</code> 是你的神經網路模型物件（由 <code>Net()</code> 建立，並載入了已訓練的權重）。當你把 <code>images</code> 傳進 <code>net()</code> 時，PyTorch 會自動呼叫 <code>Net</code> 類別裡定義的 <code>forward()</code> 函式。也就是說，這行程式會讓圖片依序通過：</p>
<ol>
<li><p>卷積層（Conv2d） → 萃取影像特徵；</p>
</li>
<li><p>ReLU 激活函式 → 引入非線性；</p>
</li>
<li><p>池化層（MaxPool2d） → 縮小影像尺寸；</p>
</li>
<li><p>全連接層（Linear） → 將特徵轉換成分類分數。</p>
</li>
</ol>
<p>這些層會依照你在 <code>forward()</code> 方法中定義的流程一層層運算，最終輸出模型對每張圖片的分類結果。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_, predicted = torch.max(outputs, 1)</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">print(&#x27;Predicted: &#x27;, &#x27; &#x27;.join(f&#x27;&#123;classes[predicted[j]]:5s&#125;&#x27;</span><br><span class="line">                              for j in range(4)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">_, predicted = torch.max(outputs, 1)</span><br><span class="line"></span><br><span class="line">print(&#x27;Predicted: &#x27;, &#x27; &#x27;.join(f&#x27;&#123;classes[predicted[j]]:5s&#125;&#x27;</span><br><span class="line">                              for j in range(4)))</span><br></pre></td></tr></table></figure>

<p>這裡使用了 PyTorch 的 <code>torch.max()</code> 函式，它的功能是<strong>找出張量（tensor）中每一列的最大值以及它所在的位置</strong>。<br><code>outputs</code> 是模型對四張圖片的預測結果，形狀是 <code>(4, 10)</code>，其中：</p>
<ul>
<li><p>4 代表輸入的圖片數量；</p>
</li>
<li><p>10 代表 CIFAR-10 的十個分類（例如飛機、汽車、貓、狗等）。</p>
</li>
</ul>
<p>對於每張圖片（每一列），模型會輸出 10 個數值，這些數值代表「模型認為該圖片屬於每個類別的信心分數」。<br><code>torch.max(outputs, 1)</code> 的第二個參數 <code>1</code> 表示沿著「第 1 維」進行操作，也就是在每一列（每張圖片的 10 個分數）中尋找最大值。</p>
<p><img src="/images/CleanShot-2025-10-29-at-11.47.42@2x-1024x85.png"></p>
<p>這個函式會回傳兩個結果：</p>
<ol>
<li><p>最大值本身（這裡用 <code>_</code> 省略掉，因為我們只關心位置而不需要數值）；</p>
</li>
<li><p>最大值的索引位置，也就是每張圖片信心分數最高的類別代號。</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">correct = 0</span><br><span class="line">total = 0</span><br><span class="line"># since we&#x27;re not training, we don&#x27;t need to calculate the gradients for our outputs</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    for data in testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        # calculate outputs by running images through the network</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        # the class with the highest energy is what we choose as prediction</span><br><span class="line">        _, predicted = torch.max(outputs, 1)</span><br><span class="line">        total += labels.size(0)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">print(f&#x27;Accuracy of the network on the 10000 test images: &#123;100 * correct // total&#125; %&#x27;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">correct = 0</span><br><span class="line">total = 0</span><br><span class="line"># since we&#x27;re not training, we don&#x27;t need to calculate the gradients for our outputs</span><br><span class="line">with torch.no_grad():</span><br><span class="line">    for data in testloader:</span><br><span class="line">        images, labels = data</span><br><span class="line">        # calculate outputs by running images through the network</span><br><span class="line">        outputs = net(images)</span><br><span class="line">        # the class with the highest energy is what we choose as prediction</span><br><span class="line">        _, predicted = torch.max(outputs, 1)</span><br><span class="line">        total += labels.size(0)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">print(f&#x27;Accuracy of the network on the 10000 test images: &#123;100 * correct // total&#125; %&#x27;)</span><br></pre></td></tr></table></figure>

<p><code>correct</code> 用來計算預測正確的圖片數量，<code>total</code> 則用來計算測試資料的總張數。最終會用這兩個數字來計算正確率。</p>
<p>接著的 <code>with torch.no_grad():</code> 是 PyTorch 中常見的寫法，表示在這個區塊中<strong>不需要追蹤梯度（gradients）</strong>。在測試或推論階段，我們並不需要做反向傳播（backpropagation）或更新權重，因此可以關閉梯度計算來節省記憶體與運算時間。這樣能讓模型運行得更快，也避免誤修改權重。</p>
<p>在程式中，<code>for data in testloader:</code> 會從測試資料載入器 <code>testloader</code> 中依序取出資料。每次取出的 <code>data</code> 代表一個批次（batch），其中包含四張測試圖片（<code>images</code>）及它們的真實標籤（<code>labels</code>）。接著，執行 <code>outputs = net(images)</code>，讓模型對這些圖片進行<strong>前向傳播（forward pass）</strong>，輸出每張圖片對應十個類別的分數（logits）。這些分數代表模型對每個類別的信心程度，分數越高表示模型越傾向認為圖片屬於該類別。</p>
<p>之後的這行 <code>_, predicted = torch.max(outputs, 1)</code> 用來從模型的輸出中，取出每張圖片分數最高的那一個類別，作為模型的預測結果。這裡的 <code>predicted</code> 是一個包含四個數字的張量（tensor），每個數字代表模型認為圖片所屬的類別索引（例如 0 代表「plane」、1 代表「car」等）。換句話說，這一步是將模型的「信心分數」轉換成實際的「分類決策」。</p>
<p>接著兩行 <code>total += labels.size(0)</code> 和 <code>correct += (predicted == labels).sum().item()</code> 是用來統計模型的預測表現。<code>labels.size(0)</code> 代表這一批圖片的數量（通常是 4），會加到 <code>total</code> 中，以累計測試圖片的總數。而 <code>(predicted == labels)</code> 會比較模型預測的類別與真實標籤是否一致，產生一個布林值陣列（例如 <code>[True, False, True, True]</code>）。使用 <code>.sum()</code> 會將其中的 <code>True</code> 視為 1、<code>False</code> 視為 0，相加後就能得到這一批中模型預測正確的圖片數。最後，<code>.item()</code> 會把這個張量轉換成普通的整數，再加到 <code>correct</code> 裡，表示目前累積的正確預測數量。</p>
<p>當迴圈跑完所有測試資料後，模型就已經對整個測試集（共 10,000 張圖片）完成預測。</p>
<p><img src="/images/CleanShot-2025-10-29-at-11.57.50@2x-1024x45.png"></p>
<p>代表模型在 10,000 張測試圖片中，有 53% 被正確分類。</p>
<p><img src="/images/CleanShot-2025-10-29-at-11.59.02@2x-1024x84.png"></p>
<p>這裡的 <code>100 * correct // total</code> 使用整數除法，讓結果以百分比的形式輸出。</p>
<h2 id="Training-on-GPU"><a href="#Training-on-GPU" class="headerlink" title="Training on GPU"></a>Training on GPU</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&#x27;cuda:0&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line"># Assuming that we are on a CUDA machine, this should print a CUDA device:</span><br><span class="line">&lt;div&gt;&lt;/div&gt;</span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(&#x27;cuda:0&#x27; if torch.cuda.is_available() else &#x27;cpu&#x27;)</span><br><span class="line"></span><br><span class="line"># Assuming that we are on a CUDA machine, this should print a CUDA device:</span><br><span class="line"></span><br><span class="line">print(device)</span><br></pre></td></tr></table></figure>

<p><code>device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)</code> 利用了 PyTorch 的 <code>torch.cuda.is_available()</code> 函式來檢查系統中是否安裝並啟用了 CUDA。若電腦中有支援的 NVIDIA 顯示卡且 CUDA 可用，這個函式會回傳 <code>True</code>，程式便會選擇 <code>&#39;cuda:0&#39;</code> 作為運算裝置，代表使用第 0 張 GPU（即第一張 GPU 卡）進行運算。若 CUDA 不可用，則回傳 <code>False</code>，程式會改用 <code>&#39;cpu&#39;</code>，也就是中央處理器進行運算。<code>torch.device()</code> 則會依據這個結果建立一個「裝置物件（device object）」，方便之後將模型或資料移動到指定的裝置上執行。</p>
<p>接著這行 <code>print(device)</code> 會印出目前使用的運算裝置。如果系統有支援 CUDA 且 PyTorch 安裝的是 GPU 版本，輸出結果會是 <code>cuda:0</code>，表示模型將在 GPU 上執行；若系統中沒有可用的 GPU，則輸出為 <code>cpu</code>，代表程式會使用 CPU 進行運算。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.to(device)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.to(device)</span><br></pre></td></tr></table></figure>

<p>當你執行 <code>net.to(device)</code> 時，PyTorch 會把模型中所有的參數（包含權重 weights 與偏差 biases）從預設的 CPU 記憶體移動到目標裝置上。如果系統有支援 CUDA 且 GPU 可用，模型就會被移到顯示卡中運行；若沒有 GPU，則會保留在 CPU 上。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = data[0].to(device), data[1].to(device)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inputs, labels = data[0].to(device), data[1].to(device)</span><br></pre></td></tr></table></figure>

<p><code>data[0]</code> 代表這一批的輸入圖片（影像張量），而 <code>data[1]</code> 則是對應的真實標籤（例如這張圖片是「貓」或「狗」）。這兩者最初都會存在 CPU 記憶體中，因為資料載入的預設裝置就是 CPU。</p>
<p><code>to(device)</code> 是 PyTorch 用來<strong>把 Tensor 傳送到指定裝置</strong>的方法。如果前面設定的 <code>device</code> 是 <code>&#39;cuda:0&#39;</code>，那這行指令會把 <code>inputs</code> 和 <code>labels</code> 都傳送到 GPU 記憶體中；如果沒有可用的 GPU，則會留在 CPU 上。這樣做可以確保模型（<code>net</code>）與資料位於相同的運算環境中，從而讓後續的運算（例如前向傳播、損失計算、反向傳播等）能夠正常執行。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://stackpenguin.com">StackPenguin</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章連結: </span><span class="post-copyright-info"><a href="https://stackpenguin.com/2025/10/29/training-a-classifier/">https://stackpenguin.com/2025/10/29/training-a-classifier/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版權聲明: </span><span class="post-copyright-info">本部落格所有文章除特別聲明外，均採用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 授權協議。轉載請註明來源 <a href="https://stackpenguin.com" target="_blank">StackPenguin</a>！</span></div></div><div class="tag_share"><div class="post-share"><div class="social-share" data-image="/images/Pytorch.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/11/05/linode-docker-n8n/" title="如何在 Linode 用 Docker 架設 n8n"><img class="cover" src="/images/n8n-linode.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">如何在 Linode 用 Docker 架設 n8n</div></div><div class="info-2"><div class="info-item-1">下面是整趟 n8n on Linode + Cloudflare Tunnel 部署旅程完整總整理包含每一步、每個踩雷點、每個修法。 Linode 上部署： Docker + Docker Compose  n8n with persistent volume  Cloudflare Tunnel (免開 port、HTTPS、domain)  安全保護（BasicAuth）   最終網址 : https://n8n.stackpenguin.com (根據你自己的網域) 系統前置SSH 進 Linode → 更新1sudo apt update &amp;&amp; sudo apt upgrade -y  1sudo apt update &amp;&amp; sudo apt upgrade -y  安裝 Docker &amp; Compose（先確認再裝）檢查 Docker1docker --version  1docker --version  沒有就安裝12curl -fsSL https://get.docker.com | sudo bashsudo userm...</div></div></div></a><a class="pagination-related" href="/2025/10/26/yolov9-practice/" title="Yolov9 實作練習"><img class="cover" src="/images/YOLOv9.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Yolov9 實作練習</div></div><div class="info-2"><div class="info-item-1">內容全參考影片 : https://www.youtube.com/watch?v=tMwyxKttZd0&t=60s  首先建立一個Conda的虛擬環境，之後進入後先下載labelImg。 1(Labelimg) C:\Users\pudy6&gt;pip install labelImg  1(Labelimg) C:\Users\pudy6&gt;pip install labelImg  如果labelImg閃退，建議不用進到虛擬環境，直接去下載LabelImg。  建立一個dataset資料夾 123456789dataset├─images│  ├─test(測試集)│  ├─train(訓練集)│  └─val(驗證集)└─labels    ├─test    ├─train    └─val  123456789dataset├─images│  ├─test(測試集)│  ├─train(訓練集)│  └─val(驗證集)└─labels    ├─test    ├─train    └─val  8:1:1  點擊Open Dir打開訓練集的圖片資料夾。 ...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/Avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">StackPenguin</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">30</div></a><a href="/tags/"><div class="headline">標籤</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分類</div><div class="length-num">35</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/KingKaiZhuang"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons"><a class="social-icon" href="https://www.instagram.com/zhuang_2024/" target="_blank" title="Instagram"><i class="fab fa-instagram" style="color: #E4405F;"></i></a><a class="social-icon" href="https://line.me/ti/p/EeWpQ7ef-e" target="_blank" title="LINE"><i class="fab fa-line" style="color: #00C300;"></i></a><a class="social-icon" href="https://www.facebook.com/profile.php?id=100002212333597" target="_blank" title="Facebook"><i class="fab fa-facebook" style="color: #4267B2;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><p>🏫 STUST</p>
<p>📖 正在準備 <strong>CPE</strong> 和 <strong>多益</strong>。</p>
</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目錄</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A3%9C%E5%85%85-ReLU-%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">1.</span> <span class="toc-text">補充 : ReLU 的作用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Train-the-network"><span class="toc-number"></span> <span class="toc-text">Train the network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Training-on-GPU"><span class="toc-number"></span> <span class="toc-text">Training on GPU</span></a></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2026/02/12/11321%20Sort!%20Sort!!%20And%20Sort!!!/" title="UVA 11321 Sort! Sort!! And Sort!!!"><img src="/images/CPE.webp" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="UVA 11321 Sort! Sort!! And Sort!!!"/></a><div class="content"><a class="title" href="/2026/02/12/11321%20Sort!%20Sort!!%20And%20Sort!!!/" title="UVA 11321 Sort! Sort!! And Sort!!!">UVA 11321 Sort! Sort!! And Sort!!!</a><time datetime="2026-02-12T06:25:56.000Z" title="發表於 2026-02-12 14:25:56">2026-02-12</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/04/unity-scene-data/" title="Unity 場景切換以及 Data 共享"><img src="/images/unity-data-scene.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Unity 場景切換以及 Data 共享"/></a><div class="content"><a class="title" href="/2025/12/04/unity-scene-data/" title="Unity 場景切換以及 Data 共享">Unity 場景切換以及 Data 共享</a><time datetime="2025-12-03T16:00:00.000Z" title="發表於 2025-12-04 00:00:00">2025-12-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/12/01/streamlit-yolo/" title="Streamlit 結合 yolo 辨識圖片"><img src="/images/streamlit_yolo.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Streamlit 結合 yolo 辨識圖片"/></a><div class="content"><a class="title" href="/2025/12/01/streamlit-yolo/" title="Streamlit 結合 yolo 辨識圖片">Streamlit 結合 yolo 辨識圖片</a><time datetime="2025-11-30T16:00:00.000Z" title="發表於 2025-12-01 00:00:00">2025-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/11/26/next-js-v16-base/" title="Next.js v16 基本常識"><img src="/images/Gemini_Generated_Image_8zuhop8zuhop8zuh-1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Next.js v16 基本常識"/></a><div class="content"><a class="title" href="/2025/11/26/next-js-v16-base/" title="Next.js v16 基本常識">Next.js v16 基本常識</a><time datetime="2025-11-25T16:00:00.000Z" title="發表於 2025-11-26 00:00:00">2025-11-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2025/11/24/interest-and-yield/" title="利率＆殖利率"><img src="/images/image-12.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="利率＆殖利率"/></a><div class="content"><a class="title" href="/2025/11/24/interest-and-yield/" title="利率＆殖利率">利率＆殖利率</a><time datetime="2025-11-23T16:00:00.000Z" title="發表於 2025-11-24 00:00:00">2025-11-24</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2024 - 2026 By StackPenguin</span></div><div class="footer_custom_text">StackPenguin | STUST</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="閱讀模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日夜模式切換"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="單欄和雙欄切換"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="設定"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目錄"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到頂端"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><div class="js-pjax"><script>(() => {
  const abcjsInit = () => {
    const abcjsFn = () => {
      setTimeout(() => {
        const sheets = document.querySelectorAll(".abc-music-sheet")
        for (let i = 0; i < sheets.length; i++) {
          const ele = sheets[i]
          if (ele.children.length > 0) continue

          // Parse parameters from data-params attribute
          let params = {}
          const dp = ele.getAttribute("data-params")
          if (dp) {
            try {
              params = JSON.parse(dp)
            } catch (e) {
              console.error("Failed to parse data-params:", e)
            }
          }

          // Merge parsed parameters with the responsive option
          // Ensures params content appears before responsive
          const options = { ...params, responsive: "resize" }

          // Render the music score using ABCJS.renderAbc
          ABCJS.renderAbc(ele, ele.textContent, options)
        }
      }, 100)
    }

    if (typeof ABCJS === "object") {
      abcjsFn()
    } else {
      btf.getScript("https://cdn.jsdelivr.net/npm/abcjs@6.6.0/dist/abcjs-basic-min.min.js").then(abcjsFn)
    }
  }

  if (window.pjax) {
    abcjsInit()
  } else {
    window.addEventListener("load", abcjsInit)
  }

  btf.addGlobalFn("encrypt", abcjsInit, "abcjs")
})()</script></div><div class="aplayer no-destroy"  data-id="60198" data-server="netease" data-type="playlist" data-fixed="true" data-autoplay="false"> </div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/fireworks.min.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="false"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>